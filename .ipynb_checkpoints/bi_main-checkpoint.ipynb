{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752bafcc-a215-4d17-a0e9-5af3f56e95cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9637f773-f290-4a97-8236-004d960a2097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Custom Dataset with Grouping for LOPO\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, folder_path, fixed_length=38400):\n",
    "        \"\"\"\n",
    "        Initialize the dataset, ensuring all signals are of a fixed length.\n",
    "        \n",
    "        folder_path: Path to the base directory containing subfolders (V-H, V-L, A-H, A-L).\n",
    "        fixed_length: Target length for all signals (pad or truncate to this length).\n",
    "        \"\"\"\n",
    "        self.signals = []\n",
    "        self.labels = []\n",
    "        self.groups = []\n",
    "        self.fixed_length = fixed_length  # Target length for padding/truncation\n",
    "        label_map = {'V-H': 0, 'V-L': 2, 'A-H': 1, 'A-L': 3}  # Map folder names to labels\n",
    "        \n",
    "        participant_id = 0  # Simulate grouping by participant\n",
    "        for label_name, label_value in label_map.items():\n",
    "            folder = os.path.join(folder_path, label_name)\n",
    "            for file in os.listdir(folder):\n",
    "                if file.endswith('.mat'):\n",
    "                    mat = sio.loadmat(os.path.join(folder, file))\n",
    "                    signal = self.extract_signal(mat, file)\n",
    "                    signal = self.adjust_length(signal)  # Pad or truncate to fixed length\n",
    "                    self.signals.append(self.normalize(signal))  # Normalize the signal\n",
    "                    self.labels.append(label_value)\n",
    "                    self.groups.append(participant_id)\n",
    "            participant_id += 1  # Increment participant ID for grouping\n",
    "\n",
    "        # Debug: Print label distribution\n",
    "        print(\"Label distribution:\", Counter(self.labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.signals)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return the signal and corresponding label at the given index.\n",
    "        \"\"\"\n",
    "        signal = torch.tensor(self.signals[idx], dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        return signal, label\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize(signal):\n",
    "        \"\"\"\n",
    "        Normalize the ECG signal to have zero mean and unit variance.\n",
    "        \"\"\"\n",
    "        return (signal - np.mean(signal)) / (np.std(signal) + 1e-8)\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_signal(mat, filename):\n",
    "        \"\"\"\n",
    "        Dynamically extract ECG signal data from a .mat file and ensure it is 1D.\n",
    "        \n",
    "        mat: Loaded .mat file.\n",
    "        filename: Name of the .mat file (for error reporting).\n",
    "        \n",
    "        Returns:\n",
    "        signal: Extracted ECG signal as a 1D array.\n",
    "        \"\"\"\n",
    "        # Attempt to find the largest numeric array in the .mat file\n",
    "        possible_signals = {key: mat[key] for key in mat.keys() if isinstance(mat[key], np.ndarray)}\n",
    "        if not possible_signals:\n",
    "            raise ValueError(f\"No valid numeric arrays found in {filename}. Please check the file structure.\")\n",
    "\n",
    "        # Return the array with the largest number of elements\n",
    "        signal_key = max(possible_signals, key=lambda k: possible_signals[k].size)\n",
    "        signal = possible_signals[signal_key]\n",
    "\n",
    "        # Flatten 2D signals into 1D if necessary\n",
    "        if signal.ndim > 1:\n",
    "            signal = signal.flatten()\n",
    "\n",
    "        return signal.squeeze()  # Ensure the signal is flattened\n",
    "\n",
    "    def adjust_length(self, signal):\n",
    "        \"\"\"\n",
    "        Pad or truncate the signal to match the fixed length.\n",
    "        \"\"\"\n",
    "        if len(signal) > self.fixed_length:\n",
    "            # Truncate if the signal is longer than the fixed length\n",
    "            return signal[:self.fixed_length]\n",
    "        elif len(signal) < self.fixed_length:\n",
    "            # Pad with zeros if the signal is shorter than the fixed length\n",
    "            padding = np.zeros(self.fixed_length - len(signal))\n",
    "            return np.concatenate((signal, padding))\n",
    "        return signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af9c1b8-d7ca-4e8c-9c16-f83d310c07a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Enhanced 1D-CNN Architecture for ECG Signal Feature Extraction\n",
    "class Modified1DCNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Modified1DCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.MaxPool1d(2),\n",
    "\n",
    "            nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "\n",
    "        # Automatically compute the input size for the first fully connected layer\n",
    "        dummy_input = torch.zeros(1, 1, input_size)  # Batch size=1, channel=1, length=input_size\n",
    "        with torch.no_grad():\n",
    "            conv_output = self.conv_layers(dummy_input)\n",
    "            print(f\"Output shape after convolutional layers: {conv_output.shape}\")\n",
    "            conv_output_size = conv_output.view(1, -1).shape[1]\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(conv_output_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ab7a42-76ca-4d93-8faa-05affc7442bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. LOPO Cross-Validation with CNN for Feature Extraction and SVM Classification\n",
    "def train_and_evaluate(model, dataset, groups, epochs=20, learning_rate=1e-4):\n",
    "    logo = LeaveOneGroupOut()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    all_test_labels_valence = []\n",
    "    all_predictions_valence = []\n",
    "    all_test_labels_arousal = []\n",
    "    all_predictions_arousal = []\n",
    "\n",
    "    class_counts = Counter(dataset.labels)\n",
    "    class_weights = {k: 1.0 / v for k, v in class_counts.items()}\n",
    "    sample_weights = [class_weights[label] for label in dataset.labels]\n",
    "\n",
    "    patience = 5  # Early stopping patience\n",
    "\n",
    "    for train_idx, test_idx in logo.split(dataset.signals, dataset.labels, groups):\n",
    "        # Prepare train and test subsets\n",
    "        train_data = torch.utils.data.Subset(dataset, train_idx)\n",
    "        test_data = torch.utils.data.Subset(dataset, test_idx)\n",
    "        sampler = WeightedRandomSampler([sample_weights[i] for i in train_idx], len(train_idx))\n",
    "        train_loader = DataLoader(train_data, batch_size=8, sampler=sampler)\n",
    "        test_loader = DataLoader(test_data, batch_size=8, shuffle=False)\n",
    "\n",
    "        # Train the CNN for feature extraction\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for signals, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "                signals, labels = signals.to(device), labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(signals)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "            train_losses.append(epoch_loss / len(train_loader))\n",
    "            scheduler.step()\n",
    "            print(f\"Epoch {epoch+1} Loss: {train_losses[-1]:.4f}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if train_losses[-1] < best_loss:\n",
    "                best_loss = train_losses[-1]\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "        # Extract features using the trained CNN\n",
    "        model.eval()\n",
    "        train_features, train_labels = [], []\n",
    "        test_features, test_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for signals, labels in train_loader:\n",
    "                signals = signals.to(device)\n",
    "                features = model(signals)\n",
    "                train_features.append(features.cpu().numpy())\n",
    "                train_labels.append(labels.numpy())\n",
    "            \n",
    "            for signals, labels in test_loader:\n",
    "                signals = signals.to(device)\n",
    "                features = model(signals)\n",
    "                test_features.append(features.cpu().numpy())\n",
    "                test_labels.append(labels.numpy())\n",
    "        \n",
    "        train_features = np.vstack(train_features)\n",
    "        train_labels = np.hstack(train_labels)\n",
    "        test_features = np.vstack(test_features)\n",
    "        test_labels = np.hstack(test_labels)\n",
    "\n",
    "        # Split labels into valence and arousal\n",
    "        train_labels_valence = np.where(train_labels < 2, train_labels, train_labels - 2)\n",
    "        test_labels_valence = np.where(test_labels < 2, test_labels, test_labels - 2)\n",
    "\n",
    "        train_labels_arousal = np.where(train_labels % 2 == 0, train_labels // 2, train_labels // 2)\n",
    "        test_labels_arousal = np.where(test_labels % 2 == 0, test_labels // 2, test_labels // 2)\n",
    "\n",
    "        # Train SVM on extracted features for Valence\n",
    "        svm_valence = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "        svm_valence.fit(train_features, train_labels_valence)\n",
    "        y_pred_valence = svm_valence.predict(test_features)\n",
    "        all_test_labels_valence.extend(test_labels_valence)\n",
    "        all_predictions_valence.extend(y_pred_valence)\n",
    "\n",
    "        # Train SVM on extracted features for Arousal\n",
    "        svm_arousal = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "        svm_arousal.fit(train_features, train_labels_arousal)\n",
    "        y_pred_arousal = svm_arousal.predict(test_features)\n",
    "        all_test_labels_arousal.extend(test_labels_arousal)\n",
    "        all_predictions_arousal.extend(y_pred_arousal)\n",
    "\n",
    "    # Evaluate Valence\n",
    "    print(\"Valence Classification Report:\")\n",
    "    print(classification_report(all_test_labels_valence, all_predictions_valence, target_names=['L', 'H']))\n",
    "    cm_valence = confusion_matrix(all_test_labels_valence, all_predictions_valence)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm_valence, annot=True, fmt='d', cmap='Blues', xticklabels=['L', 'H'], yticklabels=['L', 'H'])\n",
    "    plt.title('Confusion Matrix - Valence')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "    # Evaluate Arousal\n",
    "    print(\"Arousal Classification Report:\")\n",
    "    print(classification_report(all_test_labels_arousal, all_predictions_arousal, target_names=['L', 'H']))\n",
    "    cm_arousal = confusion_matrix(all_test_labels_arousal, all_predictions_arousal)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm_arousal, annot=True, fmt='d', cmap='Blues', xticklabels=['L', 'H'], yticklabels=['L', 'H'])\n",
    "    plt.title('Confusion Matrix - Arousal')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e812882f-ba00-403c-8c0c-b6f7055009ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    data_path = '/Users/lthsieh/TAMU/24Fall/ECEN649/Final_Codes/bi_GT'\n",
    "    dataset = ECGDataset(data_path)\n",
    "    groups = dataset.groups\n",
    "\n",
    "    # Define the CNN model\n",
    "    signal_length = len(dataset[0][0].squeeze())\n",
    "    model = Modified1DCNN(signal_length).to(device)\n",
    "\n",
    "    # Train and evaluate using LOPO with SVM\n",
    "    train_and_evaluate(model, dataset, groups, epochs=20, learning_rate=1e-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "piunet",
   "language": "python",
   "name": "piunet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
